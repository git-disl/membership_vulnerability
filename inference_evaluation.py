"""
@author: staceytruex
"""
from inference_attack import attack, balance_attack_data, import_model
import numpy as np
from sklearn.metrics.classification import accuracy_score, precision_score, recall_score, f1_score
from prettytable import PrettyTable
import os


# noinspection PyShadowingNames,PyBroadException
def validate_args(args):
    if args.n_runs < 1:
        print('Invalid value' + str(args.n_runs) + 'provided for n_runs.')
        return None
    runs = args.n_runs
    if args.target_train_def is None:
        print('No target_train_def provided.')
        print('This is required to generate a target model for inference attack.')
        return None
    target_model_def = import_model(args.target_train_def)
    if target_model_def is None:
        return None
    try:
        x = np.load(args.target_data_loc + 'train_x.npy', allow_pickle=True)
        y = np.load(args.target_data_loc + 'train_y.npy', allow_pickle=True)
    except:
        print('Error loading target training data from ' + args.target_data_loc + '.')
        return None
    if x is None or y is None:
        print('Error loading target training data from ' + args.target_data_loc + '.')
        return None
    target_train_data = (x, y)
    try:
        x = np.load(args.target_data_loc + 'test_x.npy', allow_pickle=True)
        y = np.load(args.target_data_loc + 'test_y.npy', allow_pickle=True)
    except:
        print('Error loading target testing data from ' + args.target_data_loc + '.')
        return None
    if x is None or y is None:
        print('Error loading target testing data from ' + args.target_data_loc + '.')
        return None
    target_test_data = (x, y)
    try:
        x = np.load(args.attacker_data_loc + 'data_x.npy', allow_pickle=True)
        y = np.load(args.attacker_data_loc + 'data_y.npy', allow_pickle=True)
    except:
        print('Error loading attacker data from ' + args.attacker_data_loc)
        return None
    if x is None or y is None:
        print('Error loading attacker data from ' + args.attacker_data_loc)
        return None
    attacker_data = (x, y)
    n_shadow = args.n_shadow
    if n_shadow is None:
        print('Number of shadow models was not specified. Setting to 10.')
        n_shadow = 10
    if args.shadow_train_def is None:
        print('No shadow training file was provided. Will use target model training file.')
        shadow_model_def = args.target_train_def
    else:
        shadow_model_def = args.shadow_train_def
    shadow_model = import_model(args.shadow_train_def)
    if shadow_model is None:
        return None
    attack_model_def = import_model(args.attack_train_def)
    if attack_model_def is None:
        return None
    labels = args.labels
    _, y = target_train_data
    if (labels is None) or (len(labels) != len(np.unique(y))):
        labels = np.unique(y)

    return {'runs': runs, 'target_model_def': target_model_def, 'target_train_data': target_train_data,
            'target_test_data': target_test_data, 'attacker_data': attacker_data, 'n_shadow': n_shadow,
            'shadow_model_def': shadow_model_def, 'attack_model_def': attack_model_def, 'labels': labels}


# noinspection PyShadowingNames
def run_inference_evaluation(attack_input):
    target_model_def = attack_input['target_model_def']
    target_train_data = attack_input['target_train_data']
    target_test_data = attack_input['target_test_data']
    attacker_data = attack_input['attacker_data']
    n_shadow = attack_input['n_shadow']
    shadow_model_def = attack_input['shadow_model_def']
    attack_model_def = attack_input['attack_model_def']
    labels = attack_input['labels']

    # Train target model
    target_model_def.train(train_data=target_train_data, test_data=target_test_data)

    # Balance data for evaluation
    train_x, train_y = target_train_data
    test_x, test_y = target_test_data
    (train_x, train_y), (test_x, test_y) = balance_attack_data(train_x, train_y, test_x, test_y)

    # Get target model output for attack evaluation
    attack_test_in = target_model_def.evaluate(train_x)
    attack_test_out = target_model_def.evaluate(test_x)

    # Pair outputs with target model class
    attack_test_in = (attack_test_in, train_y)
    attack_test_out = (attack_test_out, test_y)

    # Run attack on target model outputs
    attack_results = attack(attacker_data=attacker_data, n_shadow=n_shadow, shadow_model_def=shadow_model_def,
                            attack_model_def=attack_model_def, labels=labels, attack_test_in=attack_test_in,
                            attack_test_out=attack_test_out, test_o_data=np.concatenate([train_x, test_x]))

    # Clear memory
    target_model_def.clear_model()

    return attack_results


def evaluate_results(result, target_def, n_shadow):
    attack_test_y = result['attack_test_y']
    attack_test_x = result['attack_test_x']
    preds = result['preds']
    target_y = result['target_y']
    target_x = result['target_x']
    labels = result['labels']

    #    INFORMATION ABOUT THE MODEL UNDER ATTACK
    x = PrettyTable(['Model Definition',
                     'Training Accuracy',
                     'Testing Accuracy'])
    x.float_format = ".2"

    target_preds = np.argmax(attack_test_x, axis=1)
    train_acc = accuracy_score(target_y[attack_test_y == 1], target_preds[attack_test_y == 1])
    test_acc = accuracy_score(target_y[attack_test_y == 0], target_preds[attack_test_y == 0])

    x.add_row([target_def.split(os.path.dirname(os.getcwd()))[-1],
               train_acc * 100,
               test_acc * 100])

    print(x.get_string(title='Target Model'))

    #    INFORMATION ABOUT THE OVERALL ATTACK EFFECTIVENESS
    cols = ['Num Shadow',
            'Accuracy',
            'Precision',
            'Recall',
            'F-1']
    x = PrettyTable(cols)
    x.float_format = ".2"

    p = np.argmax(preds, axis=1)
    x.add_row([n_shadow,
               accuracy_score(attack_test_y, p) * 100,
               precision_score(attack_test_y, p) * 100,
               recall_score(attack_test_y, p) * 100,
               f1_score(attack_test_y, p) * 100])
    print(x.get_string(title='Attack Aggregate'))

    # noinspection PyShadowingNames
    def by_class_evaluation(attack_test_y, target_y, p, attack_test_x, labels=None):
        if labels is None:
            labels = np.unique(target_y)

        precisions = [precision_score(attack_test_y[target_y == c], p[target_y == c]) * 100 for c in
                      np.unique(target_y)]
        accuracies = [accuracy_score(attack_test_y[target_y == c], p[target_y == c]) * 100 for c in np.unique(target_y)]
        f1_scores = [f1_score(attack_test_y[target_y == c], p[target_y == c]) * 100 for c in np.unique(target_y)]
        recalls = [recall_score(attack_test_y[target_y == c], p[target_y == c]) * 100 for c in np.unique(target_y)]
        c_train_accs = [accuracy_score(target_y[np.logical_and(target_y == c, attack_test_y == 1)],
                                       np.argmax(attack_test_x[np.logical_and(target_y == c, attack_test_y == 1)],
                                                 axis=1)) * 100
                        for c in np.unique(target_y)]
        c_test_accs = [accuracy_score(target_y[np.logical_and(target_y == c, attack_test_y == 0)],
                                      np.argmax(attack_test_x[np.logical_and(target_y == c, attack_test_y == 0)],
                                                axis=1)) * 100
                       for c in np.unique(target_y)]

        x = PrettyTable()
        x.float_format = '.2'
        x.add_column("Class", labels)
        x.add_column('Target Accuracy Train', np.round(c_train_accs, 2))
        x.add_column('Target Accuracy Test', np.round(c_test_accs, 2))
        x.add_column("Attack Precision", np.round(precisions, 2))
        x.add_column("Attack Accuracy", np.round(accuracies, 2))
        x.add_column("Attack Recall", np.round(recalls, 2))
        x.add_column("Attack F-1 Score", np.round(f1_scores, 2))
        x.add_column("Percentage of Data", np.round(np.array([len(target_y[target_y == c]) / len(target_y) * 100
                                                              for c in np.unique(target_y)]), 2))
        print(x.get_string(title='Per Class Evaluation'))

    by_class_evaluation(attack_test_y, target_y, p, attack_test_x, labels=labels)

    return {'attack_test_y': attack_test_y, 'attack_test_x': attack_test_x, 'preds': preds, 'target_y': target_y,
            'target_x': target_x, 'target_def': target_def, 'n_shadow': n_shadow, 'labels': labels}


def final_evaluation(results):
    r = results[0]
    attack_test_y = r['attack_test_y']
    attack_test_x = r['attack_test_x']
    preds = r['preds']
    target_y = r['target_y']
    target_x = r['target_x']
    target_def = r['target_def']
    n_shadow = r['n_shadow']
    labels = r['labels']
    for r in results[1:]:
        attack_test_y_r = r['attack_test_y']
        attack_test_x_r = r['attack_test_x']
        preds_r = r['preds']
        target_y_r = r['target_y']
        target_x_r = r['target_x']
        attack_test_y = np.concatenate([attack_test_y, attack_test_y_r])
        attack_test_x = np.concatenate([attack_test_x, attack_test_x_r])
        preds = np.concatenate([preds, preds_r])
        target_y, target_x = np.concatenate([target_y, target_y_r]), np.concatenate([target_x, target_x_r])
    _ = evaluate_results({'attack_test_y': attack_test_y, 'attack_test_x': attack_test_x, 'preds': preds,
                          'target_y': target_y, 'target_x': target_x, 'labels': labels}, target_def, n_shadow)


# noinspection PyShadowingNames
def conduct_inference_evaluation(runs, target_def, n_shadow, args):
    results = []
    for run in np.arange(runs):
        print('Conducting run ' + str(run + 1) + ' of ' + str(runs) + '.')
        run_result = run_inference_evaluation(args)
        print('Results of run ' + str(run + 1) + ':')
        results.append(evaluate_results(run_result, target_def, n_shadow))
    if runs > 1:
        print('Final result averaged from total of ' + str(runs) + ' runs:')
        final_evaluation(results)
    return results


# noinspection PyShadowingNames
def run_inference_experiment(args):
    attack_input = validate_args(args)
    if attack_input is not None:
        results = conduct_inference_evaluation(args.n_runs, args.target_train_def, args.n_shadow, attack_input)
    else:
        print('Due to argument error, evaluation could be not be completed.')
        return None
    return results
